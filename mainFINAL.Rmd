---
title: "House Prediction"
output:
  html_document: default
  html_notebook: default
---
##Quick look at the dataset

First we read in our data:

```{r, message=FALSE, warning=FALSE}
#stringsAsFactors is used here to be able later to convert non numerical to numerical
training.set.full <- read.csv("train.csv", stringsAsFactors=FALSE)
testing.set.full <- read.csv("test.csv", stringsAsFactors=FALSE)
```

Overview on the data
```{r, results='hide'}
head(training.set.full)
```
  
  
##Handle non numerical columns

Here we implement a function to convert all cathegorical data to numerical

```{r, message=FALSE, warning=FALSE}
#this function convert all the data to numeric
handle_Non_Numerical_columns <- function(data){
  
#for each field or column
for (field in 1:NCOL(data)){
      if (!is.numeric(data[1, field])){
        
        allValues = c()
        #creating a set of all the values of the column
        #for each value in the field
        for (value in data[, field]){
          if ((!value %in% allValues) && !is.na(value)){
            allValues = c(allValues, value)
          }
        }
        
        
        #browse the column and substitute the values by their numerical corresponding
        #for each value in the field
        pos = 1
        for (value in data[, field]){
          #the function match returns the position of value in the list allValues
          data[pos, field] = match(value, allValues)
          pos = pos + 1
          
        }
        
      }
#convert the column in numeric
data[,field] = as.numeric(data[,field])
}
  
return(data)
}

training.set.full = handle_Non_Numerical_columns(training.set.full)
testing.set.full = handle_Non_Numerical_columns(testing.set.full)
```

Now we wanna check for each column if the number of NA's is greather than the number of available values. In which case we will drop it from the column list

```{r, message=FALSE, warning=FALSE, echo=FALSE}
#number of rows in the data set
c(Number_of_rows_in_data_set = NROW(training.set.full))
```

```{r, message=FALSE, warning=FALSE}
#counting the number of NA values for each independent variable
na_count <-sapply(training.set.full, function(y) sum(length(which(is.na(y)))))

#keeping only variable with at least 1 NA value
to_remove = c()
for (i in 1:length(na_count)){
  if(na_count[[i]] == 0){
    to_remove = c(to_remove, i)
  }
}
na_count = na_count[-to_remove]
na_count
```

the first column(Id) of the data frame represent an identifier for each observation wich is useless since it has almost no relationship with the SalePrice. We are also going to drop "Alley", "PoolQC", "Fence" and "MiscFeature" since they contain more NA's than actual available values. 

```{r, message=FALSE, warning=FALSE, results='hide'}
column_to_drop = c("Id", "Alley", "PoolQC", "Fence", "MiscFeature")
```



LotFrontage and FireplaceQu have respectively 259/1460 and 690/1460 missing values. Not more than a half but enough to capture our attention. To be sure that we should keep this variables, we are going to look at their relationships with the SalePrices

```{r, message=FALSE, warning=FALSE}
par(mfrow=c(2,1))

#png(height=500, width=500, pointsize=15, file="SalePrice & LotFrontage.png")
#plot(training.set.full$SalePrice, training.set.full$LotFrontage, main="Relationship between SalePrice & LotFrontage", xlab="SalePrice", ylab="LotFrontage", pch=19)

cor(na.omit(cbind(SalePrice = training.set.full$SalePrice, LotFrontage = training.set.full$LotFrontage)))
```


```{r, message=FALSE, warning=FALSE}
#png(height=500, width=500, pointsize=15, file="SalePrice & FireplaceQ.png")
#plot(training.set.full$SalePrice, training.set.full$FireplaceQ, main="Relationship between SalePrice & FireplaceQ", xlab="SalePrice", ylab="FireplaceQ", pch=19)

cor(na.omit(cbind(SalePrice = training.set.full$SalePrice, FireplaceQ = training.set.full$FireplaceQ)))
```

The (comented) plot shows to us a relatively weak positive correlation between SalePrice and LotFrontage (0.35).
wich make sense cause the Linear feet of street connected to a property should influence only a bit on the house price. The numerical value of this correlation is about 0.35 (bad but not that bad). wich motivates us to retain it. In the other habd, the plot shows us an extremely weak relationship between FireplaceQ ans SalePrice with a numerical value of 0.04. for this reason, we are going to remove FireplaceQ from our columns list

```{r, message=FALSE, warning=FALSE}
column_to_drop = c(column_to_drop,"FireplaceQ")
```


##Dealing with NAs

At this point, all our fields are numerical but we still have a lot of NAs values. to solve this issue, we will:
first test for NA and NaN using is.na and is.nan. After what we will use a common way to deal with these invalid numbers wich is to replace them with the mean of the other available data.


```{r, message=FALSE, warning=FALSE}
#this function replaces all NAs of the dataset by the mean or median of the coresponding field
Replacing.NAs <- function(data){
    for (field in 1:NCOL(data)){
          #compute the mean
              field.mean <- median(data[,field], na.rm=TRUE)
          #substitute all the NAs by the mean
               data[is.na(data[,field]), field] <- field.mean
          
    }
return(data)
}

training.set.full = Replacing.NAs(training.set.full)
testing.set.full = Replacing.NAs(testing.set.full)

```


Let's compute the correlations between features and the target variable SalePrice
```{r, message=FALSE, warning=FALSE}
correlations = data.frame(1:NCOL(training.set.full), row.names = names(training.set.full))
for (var in 1:NCOL(training.set.full)){
  var_name = names(training.set.full)[var]
  
  correlation = cor(cbind(SalePrice = training.set.full$SalePrice, Other_variable = training.set.full[,paste0(var_name, "")]))
  
  correlations[var, 1] = correlation[1,2]
  
}

colnames(correlations) = "SalePrice"
correlations = correlations[order(-correlations$SalePrice), , drop = FALSE]
head(correlations)

```


as we can see above, The most correlated variables with the SalePrice are: OverallQual, GrLivArea, GarageCars, GarageArea, TotalBsmtSF, X1stFlrSF, FullBath, TotRmsAbvGrd, YearBuilt...  
  
Let's create a list of variables heighly correlated with SalePrice

```{r, message=FALSE, warning=FALSE}
TopCorrelated = c("OverallQual", "GrLivArea", "GarageCars", "GarageArea", "TotalBsmtSF", "X1stFlrSF", "FullBath", "TotRmsAbvGrd", "YearBuilt", "YearRemodAdd", "MasVnrArea", "Fireplaces", "GarageYrBlt")
```

Now let's drop the variables in the "to_drop" list

```{r, message=FALSE, warning=FALSE}
ix <- which(!(colnames(training.set.full) %in% column_to_drop))
model_var = colnames(training.set.full)[ix]

training.set.full <- training.set.full[, model_var]
```

Before doing linear regression to predict SalePrice, we need to check the distribution and the skewness of the Response Variable(SalePrice)

```{r, message=FALSE, warning=FALSE}
library(e1071) 
#hist(training.set.full$SalePrice, main = "SalePrices Histogram", xlab = "SalePrice")
skewness(training.set.full$SalePrice)
```

We are going to apply the log transformation to make it at least approximatively normally distributed and reduce the skewness

```{r, message=FALSE, warning=FALSE}
training.set.full$LogSalePrice = log(training.set.full$SalePrice)
#hist(training.set.full$LogSalePrice, main = "LogSalePrices Histogram", xlab = "LogSalePrice")
```


###LINEAR REGRESSION

Let's build the linear model

```{r, message=FALSE, warning=FALSE}
lmf <- function(data){
  #partition data for cross validation
  set.seed(123)
  #taking randomly 0.8(80%) of the observation for training the data and 20% for cross validating
  train.index <- sample(c(1:dim(data)[1]), dim(data)[1]*0.8)
  model_lin_train = data[train.index,]
  
  model_lin_train <- subset(model_lin_train, select = -SalePrice)
  
  linreg <- lm(LogSalePrice~., data = model_lin_train)
  return(linreg)
}

linreg = lmf(training.set.full)
#summary commented to have clean output
#summary(linreg)
```



```{r, message=FALSE, warning=FALSE}
set.seed(123)
#taking randomly 0.8(80%) of the observation for training the data and 20% for cross validating
train.index <- sample(c(1:dim(training.set.full)[1]), dim(training.set.full)[1]*0.8)
model_lin_valid <- training.set.full[-train.index,]

prediction <- predict(linreg, model_lin_valid, type="response")
residuals <- model_lin_valid$LogSalePrice - prediction
c(rmse_train = sqrt(mean(linreg$residuals^2)), rmse_test = sqrt(mean(residuals^2)))
```

The RMSE here is about 0.13 for the training set and 0.15 for the test set, which is actually not that bad. But how to improve this result?  
  
If we go back and take a look to the "stars" column of the last linear model summary, we can create a list of non significant variables for this model as follow:
```{r, message=FALSE, warning=FALSE}
non_significant = c(
"MSSubClass", "MSZoning", "LotFrontage", "LotShape", "LotConfig", "LandSlope", "Condition2", "HouseStyle", "Exterior1st", "Exterior2nd", "MasVnrArea", "ExterQual", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinSF1", "BsmtFinType2", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "Heating", "Electrical", "GrLivArea", "BsmtHalfBath", "HalfBath", "BedroomAbvGr", "TotRmsAbvGrd", "GarageType", "GarageYrBlt", "GarageFinish", "GarageArea", "GarageQual", "GarageCond", "PavedDrive", "EnclosedPorch", "X3SsnPorch", "MiscVal", "MoSold", "YrSold"
)
```

We wanna check if we have some multicolinearity beyond these insignificant variables. We are then going to compute correlations between them

```{r, message=FALSE, warning=FALSE}
subset1 = subset(training.set.full, select = non_significant)

cor = cor(subset1)
#this loop used is to keep only couple of variables with a heigh correlation (greater than 0.7 and less -0.7) 
for (c in colnames(cor)){
  for(r in rownames(cor)){
    if((cor[r, c] >= 0.7 || cor[r, c] <= -0.7) && (r != c)){
      print(c(var1 = r, var2 = c, cor = cor[r, c]))
    }
  }
}
```

Here we have three couples of heighly correlated indipendent variables:  

####"Exterior1st" vs "Exterior2nd"

```{r, message=FALSE, warning=FALSE, echo=FALSE}
print("are they in the list of heighly correlated with SalePrice?") 
cbind(Exterior1st = ("Exterior1st" %in% TopCorrelated), Exterior2nd = ("Exterior2nd" %in% TopCorrelated))
print("their correlations with SalePrice")
cbind(Exterior1st = correlations["Exterior1st",], Exterior2nd = correlations["Exterior2nd",])
```

"Exterior1st" and "Exterior2nd" are heighly correlated with a correlation of about 0.74; they both represent exterior covering on house. Therefore, we are going to keep "Exterior2nd" since it has a stronger correlation with SalePrice.
```{r, message=FALSE, warning=FALSE}
training.set.full$Exterior1st = NULL
linreg <- lmf(training.set.full)
#summary(linreg)
```

We notice that removing Exterior1st, the Adjusted R-squared increased a bit; which is a sign that we probably did the right thing. However, Exterior2nd is still insignificant in this new model so we are going to remove it too

```{r, message=FALSE, warning=FALSE}
training.set.full$Exterior2nd = NULL
linreg <- lmf(training.set.full)
#summary(linreg)
```

The Adjusted R-squared increased so again, we've probably done the right thing. 

###"BsmtFinSF2" vs "BsmtFinType2"

The second couple of heigh correlated indipendent variables is "BsmtFinSF2" and "BsmtFinType2" with again a correlation of about 0.74; here we are going to remove both of them for the same reason as before.

```{r, message=FALSE, warning=FALSE}
training.set.full$BsmtFinType2 = NULL
training.set.full$BsmtFinSF2 = NULL
linreg <- lmf(training.set.full)
#summary(linreg)
#summary(linreg)
```

We've got Adjusted R-squared incresed and SaleCondition moved from almost significant to insignificant and MSZoning moved from insignificant to almost significant


###"TotRmsAbvGrd" vs "GrLivArea"

The last couple of heigh correlated indipendent variables is "TotRmsAbvGrd" and "GrLivArea" with a correlation of about 0.82; both of them are in the top correlated with SalePrice. in this case we are removing TotRmsAbvGrd since it has a lower correlation with SalePrice

```{r, message=FALSE, warning=FALSE}
training.set.full$TotRmsAbvGrd = NULL
```


```{r, message=FALSE, warning=FALSE}
#update non_significant
non_significant = c(
"MSSubClass", "LotFrontage", "LotShape", "Utilities", "LotConfig", "LandSlope", "Condition2", "HouseStyle", "MasVnrArea", "ExterQual", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinSF1", "BsmtUnfSF", "TotalBsmtSF", "Heating", "Electrical", "BsmtHalfBath", "HalfBath", "BedroomAbvGr", "GarageType", "GarageYrBlt", "GarageFinish", "GarageArea", "GarageQual", "GarageCond", "PavedDrive", "EnclosedPorch", "X3SsnPorch", "MiscVal", "MoSold", "YrSold", "SaleCondition"
)
```

Now, we are going to compute the correlation of all the remaining indipendent valiables since even if they are significant they could be highly correlated between them

```{r, message=FALSE, warning=FALSE}
subset1 = subset(training.set.full, select = -c(SalePrice, LogSalePrice))
cor = cor(subset1)

#this loop is used to keep only couple of variables with a heigh correlation (greater than 0.7 and less -0.7) 
for (c in colnames(cor)){
  for(r in rownames(cor)){
    if((cor[r, c] >= 0.7 || cor[r, c] <= -0.7) && (r != c)){
      print(c(var1 = r, var2 = c, cor = cor[r, c]))
    }
  }
}
```

###"BldgType" vs "MSSubClass"

In this first couple "BldgType" and "MSSubClass", we already know that MSSubClass is insignificant that's why we are going to remove it and see the impact on BldgType

```{r, message=FALSE, warning=FALSE}
training.set.full$MSSubClass = NULL
linreg <- lmf(training.set.full)
#summary(linreg)
```

BldgType changed from one star to two stars and the Adjusted R-squared increased. these are all good signs.  

###"GarageYrBlt" vs "YearBuilt"

here, we are going to remove GarageYrBlt since is less correlated than to SalePrice than YearBuilt

```{r, message=FALSE, warning=FALSE}
training.set.full$MSSubClass = NULL
linreg <- lmf(training.set.full)
#summary(linreg)
```

##"X1stFlrSF" vs "TotalBsmtSF"

We again drop X1stFlrSF since it's correlation with SalePrice is lower

```{r, message=FALSE, warning=FALSE}
training.set.full$X1stFlrSF = NULL
linreg <- lmf(training.set.full)
#summary(linreg)
```

###"GarageArea" vs "GarageCars"

Finally, we are also going to remove GarageArea since correlation with SalePrice is lower

```{r, message=FALSE, warning=FALSE}
#remove the BsmtFinSF2
training.set.full$GarageArea = NULL
linreg <- lmf(training.set.full)
#summary(linreg)
```

Now that the risk of multicolinearity is handled, let's see what does the final list of insignificant variables looks like 

```{r, message=FALSE, warning=FALSE}
#update non_significant
non_significant = c(
"LotFrontage", "LotShape", "Utilities", "LotConfig", "LandSlope", "Condition2", "HouseStyle", "MasVnrArea", "ExterQual", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtUnfSF", "Heating", "Electrical", "BsmtHalfBath", "HalfBath", "BedroomAbvGr", "TotRmsAbvGrd", "GarageType", "GarageYrBlt", "GarageFinish", "GarageQual", "GarageCond", "PavedDrive", "EnclosedPorch", "X3SsnPorch", "MiscVal", "MoSold", "YrSold"
)
```

```{r, message=FALSE, warning=FALSE, results='hide'}
"
training.set.full2 = training.set.full
for (c in non_significant){
  print(c)
  training.set.full2[,c] = NULL
  linreg = lmf(training.set.full2)
  commented for clean result
  print(summary(linreg))
}
"
```

Exploring the output of the above (commented) loop we decided to keep somme of the insignificant variables due to the fact that thier removal was afecting negatively the model (decreasing Adjusted R-squared ...). This is the final list of variables to remove

```{r, message=FALSE, warning=FALSE}
non_significant = c(
"LotShape", "Utilities", "Condition2", "HouseStyle", "BsmtCond", "BsmtExposure", "Electrical", "GarageFinish", "GarageQual", "MiscVal", "MoSold"
)

```

Now let's remove them and build a new linear model with the new data 

```{r, message=FALSE, warning=FALSE}
for (c in non_significant){
  training.set.full[,c] = NULL
}
```


###Outliers removal

Features heigh correlated with the target variable could sometimes present huge outliers in which case in order to improve the model, those outliers should be removed. let's check if our data present some outliers starting from the heigher correlated with SalePrice

```{r, message=FALSE, warning=FALSE, results = 'hide'}
"
par(mfrow=c(2,2))
plot(training.set.full$OverallQual, training.set.full$SalePrice)
plot(training.set.full$GrLivArea, training.set.full$SalePrice)
plot(training.set.full$GarageCars, training.set.full$SalePrice)
plot(training.set.full$TotalBsmtSF, training.set.full$SalePrice)

plot(training.set.full$FullBath, training.set.full$SalePrice)
plot(training.set.full$YearBuilt, training.set.full$SalePrice)
plot(training.set.full$YearRemodAdd, training.set.full$SalePrice)
plot(training.set.full$MasVnrArea, training.set.full$SalePrice)

plot(training.set.full$Fireplaces, training.set.full$SalePrice)
plot(training.set.full$GarageYrBlt, training.set.full$SalePrice)
"
```

Explorings the plots, the only criticals outliers we can see here are on the relationship between SalePrice and GrLivArea. Let's remove them

```{r, message=FALSE, warning=FALSE}
training.set.full = training.set.full[!(training.set.full$GrLivArea > 4000 & training.set.full$SalePrice < 300000),]
```


Now let's build a new linear model with the new data 

```{r, message=FALSE, warning=FALSE}
linreg <- lmf(training.set.full)
#summary(linreg)
```

let's see the behavior of this new model on testing data

```{r, message=FALSE, warning=FALSE}
set.seed(123)
train.index <- sample(c(1:dim(training.set.full)[1]), dim(training.set.full)[1]*0.8)
model_lin_valid <- training.set.full[-train.index,]

prediction_lm <- predict(linreg, model_lin_valid, type="response")
residuals <- model_lin_valid$LogSalePrice - prediction_lm
c(rmse_train = sqrt(mean(linreg$residuals^2)), rmse_test = sqrt(mean(residuals^2)))
```

The RMSE on the test set here is 0.1160653, which is good improvement of the previous one.

##RANDOM FOREST

Now let's try with Random forest and see if the RMSE decreases

```{r, message=FALSE, warning=FALSE}
library(randomForest)
set.seed(123)
train.index <- sample(c(1:dim(training.set.full)[1]), dim(training.set.full)[1]*0.8)
model_rf_train = training.set.full[train.index,]
model_rf_valid <- training.set.full[-train.index,]
model_rf_train <- subset(model_rf_train, select = -SalePrice)

randomForestTest <- randomForest(LogSalePrice ~ ., data = model_rf_train, ntree=500)
#plot(randomForestTest)
```

Let's do the cross validation by using the model to do prediction on the other 20% of the data in order to evaluate this model

```{r, message=FALSE, warning=FALSE}
prediction <- predict(randomForestTest, model_rf_valid, type="response")
residuals <- model_rf_valid$LogSalePrice - prediction
c(rmse_test = sqrt(mean(residuals^2)))
```

The RMSE is 0.1180354 which is not an improvement of the previous. In fact, we need to work a bit more.  
 
```{r, message=FALSE, warning=FALSE}
#varImpPlot(randomForestTest)
```

##FEATURE CREATION

```{r, message=FALSE, warning=FALSE}
library(corrplot)
training.set = training.set.full
testing.set = testing.set.full

cor.par= cor(training.set, use = "everything")
#png(height=1200, width=1500, pointsize=15, file="correlations.png")
#corrplot(cor.par, method = "shade", type="upper", sig.level = 0.01, insig = "blank")
```

From the random forest plot of variable importance we notice that the must importance variable is OverallQual. Again, From the correlation plot we realize that the price has a strong positive correlation with the overall quality. The overall quality value is between 1 to 10. we can then use it to create new features. For example, it is safe to assume that newer houses should be more expensive than older ones. So we will be using the overall quality to create new indicators.

```{r, message=FALSE, warning=FALSE}
train.II = training.set
train.II$QualLivingArea = train.II$OverallQual * train.II$GrLivArea
train.II$QualExterior = train.II$OverallQual * train.II$ExterCond
```

Let's have a look at the correlations plot again

```{r, message=FALSE, warning=FALSE}
#png(height=1200, width=1500, pointsize=15, file="trainII.png")
#corrplot(cor(train.II, use = "everything"), method = "shade", type="upper", sig.level = 0.01, insig = "blank")
```

THe plot confirm that the LogPrice it strongly positively correlated to all the new features created.  
  
here we split the new data for cross validation

```{r, message=FALSE, warning=FALSE}
set.seed(123)
train.index <- sample(c(1:dim(train.II)[1]), dim(train.II)[1]*0.8)
model_train = train.II[train.index,]
model_train = subset(model_train, select = -SalePrice)
model_test <- train.II[-train.index,]
```

###LINEAR MODEL

```{r, message=FALSE, warning=FALSE}
linreg <- lmf(train.II)
prediction_lm <- predict(linreg, model_test, type="response")
residuals <- model_test$LogSalePrice - prediction_lm
c(rmse_train = sqrt(mean(linreg$residuals^2)), rmse_test = sqrt(mean(residuals^2)))
```

Now let's train the model with the whole train set
```{r, message=FALSE, warning=FALSE}
model_train = subset(train.II, select = -SalePrice)
linreg <- lm(LogSalePrice~., data = model_train)
```

###RANDOM FOREST 

```{r, message=FALSE, warning=FALSE}
rf_mod = randomForest(LogSalePrice~., data= model_train)
prediction_rf <- predict(rf_mod, model_test)
residuals <- model_test$LogSalePrice - prediction_rf
c(rmse = sqrt(mean(residuals^2)))
```

Now let's train the model with the whole training set
```{r, message=FALSE, warning=FALSE}
model_train = subset(train.II, select = -SalePrice)
rf_mod = randomForest(LogSalePrice~., data= model_train)
```

###Combianing the two models, we get a better RMSE on kaggle.

```{r}
p = (prediction_lm + prediction_rf)/2
residuals <- model_test$LogSalePrice - p
c(rmse_test = sqrt(mean(residuals^2)))
```

##SUBMISSION

Creating a final submission file combining our two best models: linear model and random forest

```{r}
testing.set$QualLivingArea = testing.set$OverallQual * testing.set$GrLivArea
testing.set$QualExterior = testing.set$OverallQual * testing.set$ExterCond
SalePriceLM <- predict(linreg, newdata = testing.set)
SalePriceRF <- predict(rf_mod, newdata = testing.set)
SalePriceFinal <- ((SalePriceLM + SalePriceRF)/2)
SalePrice <- exp(SalePriceFinal)

y<-cbind(data.frame("Id"=1461:(1460+nrow(testing.set.full)), SalePrice))
colnames(y) = c("Id", "SalePrice")
head(y)

# Save prediction to My_Submission.csv
write.csv(y,file="pred.csv", quote = FALSE, row.names = FALSE)
```

